{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18de643-ecf9-46a9-8a27-63e801f95430",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "After storing all news titles in the CSV file, we import the file and label every single word in the title using label-studio, including \"ACTIVITY\" and \"OBJECTIVITY\" following the business case:\n",
    "1. ACTIVITY: related to deal, bid, investment, response to problems, etc.\n",
    "\n",
    "2. OBJECTIVITY: related to organization, products, competitors, business partners, etc.\n",
    "\n",
    "Continuously, we export all of the titles to txt format in ConLL2003 structure, then split the file manually into 3 sub-files with 80% for training, 14% for validation, and 6% for testing. Finally, we zip all of them in a zip file (i.e \"bbc_label_bert.zip\") and use Python (i.e \"bbc_bert_processing.py\") to process the data in a suitable structure to build the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8ebf5-97a5-4821-baa4-839d54bba48b",
   "metadata": {},
   "source": [
    "# DATA MODELING\n",
    "Here we build a BERT model to explore the sentiment of entities in BBC news title about Microsoft corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42032f25-4856-4761-83ea-4b7c8fc9fcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\anaconda\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: transformers[torch] in d:\\anaconda\\lib\\site-packages (4.32.1)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.6 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.6/43.6 kB 354.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\anaconda\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in d:\\anaconda\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\anaconda\\lib\\site-packages (from transformers[torch]) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\lib\\site-packages (from transformers[torch]) (0.3.2)\n",
      "Collecting torch!=1.12.0,>=1.9 (from transformers[torch])\n",
      "  Obtaining dependency information for torch!=1.12.0,>=1.9 from https://files.pythonhosted.org/packages/5c/01/5ab75f138bf32d7a69df61e4997e24eccad87cc009f5fb7e2a31af8a4036/torch-2.2.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.2.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Obtaining dependency information for accelerate>=0.20.3 from https://files.pythonhosted.org/packages/6c/35/b3851a3c2d3ead15099defccb50b59c1165eb24dfde298abe4091ffb6cca/accelerate-0.29.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.29.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\anaconda\\lib\\site-packages (from seqeval) (1.3.0)\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0.0,>=0.11.0->datasets)\n",
      "  Obtaining dependency information for typing-extensions>=3.7.4.3 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-0.29.1-py3-none-any.whl (297 kB)\n",
      "   ---------------------------------------- 0.0/297.3 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 81.9/297.3 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 276.5/297.3 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 297.3/297.3 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.2.2-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "   ---------------------------------------- 0.0/198.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/198.6 MB 9.6 MB/s eta 0:00:21\n",
      "   ---------------------------------------- 0.9/198.6 MB 11.9 MB/s eta 0:00:17\n",
      "   ---------------------------------------- 2.0/198.6 MB 16.3 MB/s eta 0:00:13\n",
      "   ---------------------------------------- 2.1/198.6 MB 12.2 MB/s eta 0:00:17\n",
      "    --------------------------------------- 3.6/198.6 MB 16.5 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.1/198.6 MB 19.2 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 6.8/198.6 MB 21.7 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 8.3/198.6 MB 23.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 10.0/198.6 MB 24.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.9/198.6 MB 29.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 13.2/198.6 MB 34.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 14.5/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 17.1/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 18.6/198.6 MB 36.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 20.2/198.6 MB 38.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 22.1/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 22.9/198.6 MB 34.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 25.1/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 26.9/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 28.4/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 29.6/198.6 MB 32.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 32.1/198.6 MB 36.3 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 33.9/198.6 MB 38.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 34.3/198.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 35.9/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 37.7/198.6 MB 36.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 39.8/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 39.9/198.6 MB 34.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 41.1/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 43.4/198.6 MB 29.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 44.5/198.6 MB 31.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 47.0/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 49.2/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 50.7/198.6 MB 38.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 53.2/198.6 MB 43.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 54.4/198.6 MB 43.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 57.4/198.6 MB 43.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 59.4/198.6 MB 43.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 60.9/198.6 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 61.2/198.6 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 61.3/198.6 MB 31.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 61.8/198.6 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 65.5/198.6 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 66.7/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 67.6/198.6 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 68.1/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 68.1/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 68.1/198.6 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 73.0/198.6 MB 32.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 74.8/198.6 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 75.6/198.6 MB 27.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 78.8/198.6 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 80.2/198.6 MB 50.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 81.6/198.6 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 82.4/198.6 MB 36.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 84.6/198.6 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 86.7/198.6 MB 38.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 87.8/198.6 MB 34.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 88.5/198.6 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 90.7/198.6 MB 32.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 91.0/198.6 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 92.2/198.6 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 92.7/198.6 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 94.2/198.6 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 94.2/198.6 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 94.7/198.6 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 95.3/198.6 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 98.6/198.6 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 99.4/198.6 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 100.3/198.6 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 101.6/198.6 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 102.7/198.6 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 103.5/198.6 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 104.7/198.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 104.7/198.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 104.7/198.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 105.6/198.6 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 108.3/198.6 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 109.5/198.6 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 109.8/198.6 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 109.8/198.6 MB 21.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 112.4/198.6 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 115.5/198.6 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.3/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 118.8/198.6 MB 32.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 120.4/198.6 MB 50.4 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 120.6/198.6 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 123.0/198.6 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 124.2/198.6 MB 32.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.2/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.0/198.6 MB 34.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 130.0/198.6 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 131.3/198.6 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 133.2/198.6 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 134.3/198.6 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 134.6/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 134.8/198.6 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 134.9/198.6 MB 25.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 136.0/198.6 MB 24.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 137.5/198.6 MB 24.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 138.3/198.6 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 139.5/198.6 MB 21.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 141.1/198.6 MB 21.1 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 142.0/198.6 MB 21.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 142.3/198.6 MB 19.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 145.0/198.6 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 146.1/198.6 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 147.9/198.6 MB 28.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 148.8/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 149.1/198.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 151.7/198.6 MB 28.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.3/198.6 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 153.9/198.6 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 155.1/198.6 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.4/198.6 MB 28.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.1/198.6 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.9/198.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 161.3/198.6 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.4/198.6 MB 31.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 164.3/198.6 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 166.0/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 167.4/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.0/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 171.1/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 172.7/198.6 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 173.5/198.6 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 176.2/198.6 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.7/198.6 MB 38.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.0/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 179.9/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.9/198.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.5/198.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.7/198.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 182.1/198.6 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.0/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.7/198.6 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 186.5/198.6 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 187.5/198.6 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 188.5/198.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 189.1/198.6 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 191.5/198.6 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.9/198.6 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  194.6/198.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.9/198.6 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.6/198.6 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 198.6/198.6 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16189 sha256=9c7e0d4210d1c981ae98adcd8c58fba1a95eab7db541aec5e2f06a42911ca992\n",
      "  Stored in directory: c:\\users\\86158\\appdata\\local\\pip\\cache\\wheels\\bc\\92\\f0\\243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: typing-extensions, torch, seqeval, accelerate\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed accelerate-0.29.1 seqeval-1.2.2 torch-2.2.2 typing-extensions-4.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers[torch] seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0c3fb7-4f2e-4fe2-ac30-7ea9a233f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bbc_bert_processing (C:/Users/86158/.cache/huggingface/datasets/bbc_bert_processing/bbc_news_for_model/1.0.0/e63f6861124cf3d51d42aa550de7f2ee609cdc2401ad8d5a052406e45635bd4a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed63aff784741fea1bad0cafcba26d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "dataset = load_dataset(\"bbc_bert_processing.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9035a4-377e-412e-bd5c-b6ae63c4dc79",
   "metadata": {},
   "source": [
    "We use 80% for training, 14% for validation and 6% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c08e05-f443-4b05-9dcc-4285b0a89a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 103\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 17\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e281bcd-a22d-4815-bcfe-ab03d45ac5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['CAA',\n",
       "  ':',\n",
       "  'Microsoft',\n",
       "  'boss',\n",
       "  'calls',\n",
       "  'India',\n",
       "  \"'s\",\n",
       "  'new',\n",
       "  'citizenship',\n",
       "  'law',\n",
       "  \"'sad\",\n",
       "  \"'\"],\n",
       " 'ner_tags': [0, 0, 2, 2, 1, 2, 2, 0, 2, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first observation\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13fb3915-63a9-4d4e-8361-8b913989d5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=ClassLabel(names=['O', 'B-ACTIVITY', 'B-OBJECTIVITY'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "# Check the tags\n",
    "tags = dataset[\"train\"].features[f\"ner_tags\"]\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fadbde24-f3c6-4e2f-80e9-5ba93c1f221a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-ACTIVITY', 'B-OBJECTIVITY']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3e352-6304-4338-b712-6e13ba51bc81",
   "metadata": {},
   "source": [
    "### FINE-TUNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0710c47e-a4f3-4a68-bc6c-58d08e32fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "task = \"ner\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d86b00-7ad1-4e1e-a9c4-d5310872adc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce17b1af5a5640e7829394107da1862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\86158\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7a19d670854e5b97628406db8fa348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a419ad88248d4d8b893bc1d6b38ba39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3d4cd4591c4c94ae0320be55be2db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38473472-a427-4709-bee4-f41887c59d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'bill', 'gates', 'steps', 'down', 'from', 'microsoft', 'board', 'to', 'focus', 'on', 'philanthropy', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][2]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6941b6-3d3c-4bff-ab70-9f6f78ecae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 103\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 17\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7364e9-d340-4ab1-8c52-771d535136f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-ACTIVITY\",\n",
    "    2: \"B-OBJECTIVITY\"\n",
    "\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-ACTIVITY\": 1,\n",
    "    \"B-OBJECTIVITY\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4430ce3-dc9f-4b0b-b3e7-de2b29b169a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "D:\\anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29baef42e62247ba859e4081bdaff97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint,\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        num_labels=len(label_list)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d88cd4c6-c373-4f75-bb2d-3d498599a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f3efc8-feec-4fa1-ba29-ee491062a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6e31f4-ed67-4925-9106-b342ecad8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86158\\AppData\\Local\\Temp\\ipykernel_28588\\140334799.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee029473f4c4270a8f62b019c4f997f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ACTIVITY': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'OBJECTIVITY': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 5},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b30e88-b5be-4c6e-ae2c-9a5e61de0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af9391-3b11-49a9-bdb9-7cb3ce15607d",
   "metadata": {},
   "source": [
    "### TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851764b5-755b-429d-8d1a-1ba080b5e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a5d10a-3c29-4264-aca3-4a3f21113fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts NOW\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.922034</td>\n",
       "      <td>0.570470</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.598930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818315</td>\n",
       "      <td>0.648855</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.658915</td>\n",
       "      <td>0.663102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.776179</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.677291</td>\n",
       "      <td>0.679144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=0.9139618646530878, metrics={'train_runtime': 26.6781, 'train_samples_per_second': 11.583, 'train_steps_per_second': 0.787, 'total_flos': 1504324122750.0, 'train_loss': 0.9139618646530878, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training starts NOW\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadd82d-5393-4b31-96e1-f40c14d8fc0d",
   "metadata": {},
   "source": [
    "# EVALUATION AND RECOMMENDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72660aa-b828-474f-a18e-d9cd4da7ced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ACTIVITY': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 37},\n",
       " 'OBJECTIVITY': {'precision': 0.6854838709677419,\n",
       "  'recall': 0.9444444444444444,\n",
       "  'f1': 0.794392523364486,\n",
       "  'number': 90},\n",
       " 'overall_precision': 0.6854838709677419,\n",
       " 'overall_recall': 0.6692913385826772,\n",
       " 'overall_f1': 0.6772908366533864,\n",
       " 'overall_accuracy': 0.679144385026738}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a1b5d-3b31-4266-8308-741ba4283e6c",
   "metadata": {},
   "source": [
    "The model has a very good performance in identifying the \"OBJECTIVITY\" tag with almost absolute recall value. However, the metrics of the \"ACTIVITY\" tag show that it doesn't have a good prediction, that is the reason why the overall accuracy of the model is just acceptable at 66.84%. \n",
    "\n",
    "To explain, the number metric shows that there are just 37/107 observations that have the tag \"ACTIVITY\" while the \"OBJECTIVITY\" tag appear in 90 observations. It is clear that the dataset I collected mainly related to \"OBJECTIVITY\" and doesn't have enough observations to identify the tag \"ACTIVITY\", but there might be 2 more possibilities:\n",
    "\n",
    "1. Most of the news about Microsoft are just related to \"OBJECTIVITY\", no matter how many titles are there that I collect.\n",
    "   \n",
    "2. I was biased when labeling the tags for the title of the news.\n",
    "\n",
    "To enhance the performance of model, I can double-check the labeling process with my group members or define another business case to decide on a different labeling process. However, my labeling decision is not suitable to build the BERT model for this business case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f6d83c-1cc4-4771-9ef0-9bad4180259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05d9f11-874c-4a86-8a90-3b14b5e8095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TokenClassificationPipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert_model\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a753f304-3a0e-4ddf-9830-8046e0b9c38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'OBJECTIVITY',\n",
       "  'score': 0.8046711,\n",
       "  'word': 'microsoft',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'OBJECTIVITY',\n",
       "  'score': 0.7521492,\n",
       "  'word': 'xbox',\n",
       "  'start': 20,\n",
       "  'end': 24},\n",
       " {'entity_group': 'OBJECTIVITY',\n",
       "  'score': 0.5864086,\n",
       "  'word': 'executive',\n",
       "  'start': 25,\n",
       "  'end': 34},\n",
       " {'entity_group': 'OBJECTIVITY',\n",
       "  'score': 0.6137433,\n",
       "  'word': 'gaming',\n",
       "  'start': 50,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE = \"Microsoft loses key Xbox executive amid continued gaming shake-up\"\n",
    "\n",
    "ner_results = nlp(EXAMPLE)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00070afd-1547-4809-b947-6f979458f99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
